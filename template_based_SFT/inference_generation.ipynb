{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/girinman/anaconda3/envs/hai-ground/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/girinman/anaconda3/envs/hai-ground/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/girinman/anaconda3/envs/hai-ground/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/girinman/anaconda3/envs/hai-ground/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/girinman/anaconda3/envs/hai-ground/lib/libcudart.so'), PosixPath('/home/girinman/anaconda3/envs/hai-ground/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from dataset_factory import DatasetFactory\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import evaluate as eval_metrics\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"/home/girinman/repos/restaurant_suggestion_with_llms/template_based_SFT/checkpoints/polyglot-ko-1.3b-lora-parse_cmd/2023-05-15_05:31:19\"\n",
    "running_config = json.load(open(ckpt_dir + \"/running_config.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = running_config['model_name_or_path']\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_8bit=True,#running_config['train_in_8bit'],\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load adapters from the Hub and generate some output texts:\n",
    "\n",
    "peft_model_id = ckpt_dir\n",
    "lora_config = LoraConfig.from_pretrained(peft_model_id)\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.gpt_neox.layers.0.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.0.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.1.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.1.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.2.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.2.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.3.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.3.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.4.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.4.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.5.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.5.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.6.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.6.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.7.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.7.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.8.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.8.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.9.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.9.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.10.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.10.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.11.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.11.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.12.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.12.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.13.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.13.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.14.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.14.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.15.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.15.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.16.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.16.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.17.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.17.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.18.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.18.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.19.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.19.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.20.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.20.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.21.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.21.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.22.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.22.attention.query_key_value.lora_B\n",
      "base_model.model.gpt_neox.layers.23.attention.query_key_value.lora_A\n",
      "base_model.model.gpt_neox.layers.23.attention.query_key_value.lora_B\n"
     ]
    }
   ],
   "source": [
    "lora_state = torch.load(peft_model_id + '/adapter_model.bin')\n",
    "for n, m in model.named_modules():\n",
    "    if n+'.weight' in lora_state:\n",
    "        print(n)\n",
    "        m.load_state_dict({'weight' :lora_state[n+'.weight']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/girinman/.cache/huggingface/datasets/csv/default-b08ba15b77f0c96d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce21362088d440db6cc44975146c549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function CMD.get_prompt_dataset.<locals>.generate_prompt at 0x7f8748271040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c3ac4b02e243adbdf022241ee76c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ea4a63ad2e4c03ac52481f7a5e5812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/96 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "df = DatasetFactory(tokenizer)\n",
    "\n",
    "loaded_dataset = df.load_dataset(\n",
    "    name=running_config['dataset_name'],\n",
    "    max_len=running_config['max_len'],\n",
    "    batch_size=4#running_config['per_device_eval_batch_size'],\n",
    ")\n",
    "\n",
    "data = loaded_dataset.dataset\n",
    "suffix = loaded_dataset.suffix\n",
    "max_label_len = loaded_dataset.max_label_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = loaded_dataset.get_test_dataloader()\n",
    "for batch in test_loader:\n",
    "    first_batch = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/girinman/anaconda3/envs/hai-ground/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/girinman/anaconda3/envs/hai-ground/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:220: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/TensorCompare.cpp:413.)\n",
      "  attn_scores = torch.where(causal_mask, attn_scores, mask_value)\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.amp.autocast():\n",
    "    output_tokens = model.generate(\n",
    "        input_ids=first_batch['input_ids'].to(model.device),\n",
    "        max_new_tokens=88,\n",
    "        eos_token_id = tokenizer.eos_token_id,\n",
    "        pad_token_id = tokenizer.pad_token_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명령: 강남역 주변에 괜찮은 한식집 있을까요?\n",
      "시간 : 알 수 없음 알 수 없음지역 : 서울특별시 강남구 신사동 가로수길 알 수 없음\n",
      "시간 : 알 수 없음\n",
      "지역 : 알 수 없음\n",
      "시간 : 알 수 없음\n",
      "지역 : 알 수 없음\n",
      "요구사항 : 알 수 없음\n",
      "지역 : 알 수 없음\n",
      "요구사항 : 알 수 없음\n",
      "지역 : 알 수 없음\n",
      "요구사항 : 알 수 없음\n",
      "\n",
      "명령: 성신여대 근처에 혼술하기 좋은 맥주집 추천해주세요.\n",
      "시간 : 알 수 없음\n",
      "시간 : 알 수 없음\n",
      "지역 : 알 수 없음\n",
      "시간 : 알 수 없음\n",
      "지역 : 알 수 없음\n",
      "요구사항 : 알 수 없음\n",
      "지역 : 알 수 없음\n",
      "요구사항 : 알 수 없음\n",
      "지역 : 알 수 없음\n",
      "요구사항 : 알 수 없음\n",
      "지역 : 알 수 없음\n",
      "요구사항 : 알 수 없음\n",
      "\n",
      "명령: 서울 근교로 드라이브 갈만한 곳을 찾고 있어요. 어디가 좋을까요?\n",
      "시간 : 알 수 없음\n",
      "지역 : 서울특별시 강서구청역\n",
      "시간 : 알 수 없음\n",
      "지역 : 서울 강서구청역\n",
      "시간 : 알 수 없음\n",
      "지역 : 서울 강서구청역\n",
      "요구사항 : 알 수 없음\n",
      "지역 : 서울 강서구청역\n",
      "요구사항 : 알 수 없음\n",
      "\n",
      "명령: 강남역에서 소개팅 할만한 곳 추천해주세요.\n",
      "시간 : 알 수 없음\n",
      "지역 : 강남역 근처 커피숍, 커피숍, 커피숍 등\n",
      "시간 : 알 수 없음\n",
      "지역 : 강남역\n",
      "시간 : 알 수 없음\n",
      "요구사항 : 커피숍, 커피숍\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "for txt in generated:\n",
    "    print(txt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate as eval_metrics\n",
    "rouge = eval_metrics.load('rouge')\n",
    "\n",
    "def validation_step(model, tokenizer, batch, first):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=batch['input_ids'].to(model.device),\n",
    "            attention_mask=batch['attention_mask'].to(model.device),\n",
    "            max_new_tokens=max_label_len,\n",
    "            eos_token_id = tokenizer.eos_token_id,\n",
    "            pad_token_id = tokenizer.pad_token_id,\n",
    "        )\n",
    "    generated_txt = []\n",
    "    for i, g in enumerate(generated_ids):\n",
    "        decoded_txt = tokenizer.decode(g.tolist(), skip_special_tokens=True).split(suffix)\n",
    "        generated_txt.append(decoded_txt[-1].strip())\n",
    "    \n",
    "    labels = batch['decoded_labels']\n",
    "\n",
    "    return {'generated': generated_txt, 'labels': labels}\n",
    "\n",
    "def validation_epoch_end(outputs):\n",
    "    generated_txt = []\n",
    "    labels = []\n",
    "    preds = []\n",
    "\n",
    "    for i in outputs:\n",
    "        generated_txt.extend(i['generated'])\n",
    "        labels.extend(i['labels'])\n",
    "    \n",
    "    metrics = rouge.compute(predictions=generated_txt,\n",
    "                            references=labels,\n",
    "                            # tokenizer=lambda x: x.split(),\n",
    "                            )\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def evaluate(eval_dataloader):\n",
    "    eval_results = []\n",
    "    for i, batch in tqdm(enumerate(eval_dataloader), \"Generating predictions\", total=len(eval_dataloader)):\n",
    "        eval_results.append(validation_step(model, tokenizer, batch, i == 0))\n",
    "    return validation_epoch_end(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Evaluation begins***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 205/205 [19:04<00:00,  5.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Evaluation results***\n",
      "rouge1: 0.518373921879556\n",
      "rouge2: 0.2775653146251903\n",
      "rougeL: 0.4383280042643704\n",
      "rougeLsum: 0.4382960341521835\n"
     ]
    }
   ],
   "source": [
    "print(f\"***Evaluation begins***\")\n",
    "metrics = evaluate(test_loader)\n",
    "print(f\"***Evaluation results***\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Evaluation begins***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 205/205 [19:25<00:00,  5.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Evaluation results***\n",
      "rouge1: 0.5230088067234611\n",
      "rouge2: 0.2892967975202029\n",
      "rougeL: 0.44118840842498985\n",
      "rougeLsum: 0.4407998007187565\n"
     ]
    }
   ],
   "source": [
    "val_loader = loaded_dataset.get_eval_dataloader()\n",
    "\n",
    "print(f\"***Evaluation begins***\")\n",
    "metrics = evaluate(val_loader)\n",
    "print(f\"***Evaluation results***\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hai-ground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
